{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename_day_data_nov = 'C:/program2/tensorflow/data/day_data_nov.csv'# 0:00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 2., 2., ..., 2., 2., 2.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "day_data = np.ones((4320))\n",
    "day_data = day_data*2\n",
    "day_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 2., 2., ..., 1., 1., 1.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "day_data[144:432] = 1\n",
    "day_data[1152:1440] = 1\n",
    "day_data[2160:2448] = 1\n",
    "day_data[3168:3456] = 1\n",
    "day_data[4176:4320] = 1\n",
    "day_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(day_data).to_csv(filename_day_data_nov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4290</th>\n",
       "      <td>4290</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4291</th>\n",
       "      <td>4291</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4292</th>\n",
       "      <td>4292</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4293</th>\n",
       "      <td>4293</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4294</th>\n",
       "      <td>4294</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4295</th>\n",
       "      <td>4295</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4296</th>\n",
       "      <td>4296</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4297</th>\n",
       "      <td>4297</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4298</th>\n",
       "      <td>4298</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4299</th>\n",
       "      <td>4299</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4300</th>\n",
       "      <td>4300</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4301</th>\n",
       "      <td>4301</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4302</th>\n",
       "      <td>4302</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4303</th>\n",
       "      <td>4303</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4304</th>\n",
       "      <td>4304</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4305</th>\n",
       "      <td>4305</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4306</th>\n",
       "      <td>4306</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4307</th>\n",
       "      <td>4307</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4308</th>\n",
       "      <td>4308</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4309</th>\n",
       "      <td>4309</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4310</th>\n",
       "      <td>4310</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4311</th>\n",
       "      <td>4311</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4312</th>\n",
       "      <td>4312</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4313</th>\n",
       "      <td>4313</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4314</th>\n",
       "      <td>4314</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4315</th>\n",
       "      <td>4315</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4316</th>\n",
       "      <td>4316</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4317</th>\n",
       "      <td>4317</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4318</th>\n",
       "      <td>4318</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4319</th>\n",
       "      <td>4319</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4320 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0    0\n",
       "0              0  2.0\n",
       "1              1  2.0\n",
       "2              2  2.0\n",
       "3              3  2.0\n",
       "4              4  2.0\n",
       "5              5  2.0\n",
       "6              6  2.0\n",
       "7              7  2.0\n",
       "8              8  2.0\n",
       "9              9  2.0\n",
       "10            10  2.0\n",
       "11            11  2.0\n",
       "12            12  2.0\n",
       "13            13  2.0\n",
       "14            14  2.0\n",
       "15            15  2.0\n",
       "16            16  2.0\n",
       "17            17  2.0\n",
       "18            18  2.0\n",
       "19            19  2.0\n",
       "20            20  2.0\n",
       "21            21  2.0\n",
       "22            22  2.0\n",
       "23            23  2.0\n",
       "24            24  2.0\n",
       "25            25  2.0\n",
       "26            26  2.0\n",
       "27            27  2.0\n",
       "28            28  2.0\n",
       "29            29  2.0\n",
       "...          ...  ...\n",
       "4290        4290  1.0\n",
       "4291        4291  1.0\n",
       "4292        4292  1.0\n",
       "4293        4293  1.0\n",
       "4294        4294  1.0\n",
       "4295        4295  1.0\n",
       "4296        4296  1.0\n",
       "4297        4297  1.0\n",
       "4298        4298  1.0\n",
       "4299        4299  1.0\n",
       "4300        4300  1.0\n",
       "4301        4301  1.0\n",
       "4302        4302  1.0\n",
       "4303        4303  1.0\n",
       "4304        4304  1.0\n",
       "4305        4305  1.0\n",
       "4306        4306  1.0\n",
       "4307        4307  1.0\n",
       "4308        4308  1.0\n",
       "4309        4309  1.0\n",
       "4310        4310  1.0\n",
       "4311        4311  1.0\n",
       "4312        4312  1.0\n",
       "4313        4313  1.0\n",
       "4314        4314  1.0\n",
       "4315        4315  1.0\n",
       "4316        4316  1.0\n",
       "4317        4317  1.0\n",
       "4318        4318  1.0\n",
       "4319        4319  1.0\n",
       "\n",
       "[4320 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = pd.read_csv(filename_day_data_nov)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\software\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:122: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=1)`\n",
      "D:\\software\\Anaconda\\lib\\site-packages\\keras\\models.py:939: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compile time: 0.03125309944152832\n",
      "Train on 3246 samples, validate on 171 samples\n",
      "Epoch 1/50\n",
      "3246/3246 [==============================] - 9s 3ms/step - loss: 14.0879 - val_loss: 1.7242\n",
      "Epoch 2/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 2.7393 - val_loss: 1.0778\n",
      "Epoch 3/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 1.7320 - val_loss: 0.7740\n",
      "Epoch 4/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 1.4890 - val_loss: 0.9236\n",
      "Epoch 5/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 1.2029 - val_loss: 0.1400\n",
      "Epoch 6/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 0.9467 - val_loss: 0.9671\n",
      "Epoch 7/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 0.8893 - val_loss: 0.0982\n",
      "Epoch 8/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 0.9151 - val_loss: 0.2917\n",
      "Epoch 9/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 0.7557 - val_loss: 0.1206\n",
      "Epoch 10/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 0.7563 - val_loss: 0.1226\n",
      "Epoch 11/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 0.7638 - val_loss: 0.0757\n",
      "Epoch 12/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 0.7441 - val_loss: 0.1436\n",
      "Epoch 13/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 0.6945 - val_loss: 0.0998\n",
      "Epoch 14/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 0.6330 - val_loss: 0.3034\n",
      "Epoch 15/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 0.6936 - val_loss: 0.1376\n",
      "Epoch 16/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 0.6345 - val_loss: 0.0856\n",
      "Epoch 17/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 0.6418 - val_loss: 0.0446\n",
      "Epoch 18/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 0.5685 - val_loss: 0.2469\n",
      "Epoch 19/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 0.6268 - val_loss: 0.0753\n",
      "Epoch 20/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 0.5908 - val_loss: 0.5617\n",
      "Epoch 21/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 0.6209 - val_loss: 0.3450\n",
      "Epoch 22/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 0.5338 - val_loss: 0.0705\n",
      "Epoch 23/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 0.5806 - val_loss: 0.2061\n",
      "Epoch 24/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 0.5940 - val_loss: 0.0876\n",
      "Epoch 25/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 0.5250 - val_loss: 0.2911\n",
      "Epoch 26/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 0.5669 - val_loss: 0.0682\n",
      "Epoch 27/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 0.5787 - val_loss: 0.0513\n",
      "Epoch 28/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 0.5324 - val_loss: 0.1131\n",
      "Epoch 29/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 0.5271 - val_loss: 0.1402\n",
      "Epoch 30/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 0.5038 - val_loss: 0.1886\n",
      "Epoch 31/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 0.5071 - val_loss: 0.0923\n",
      "Epoch 32/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 0.5548 - val_loss: 0.2048\n",
      "Epoch 33/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 0.5385 - val_loss: 0.0863\n",
      "Epoch 34/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 0.4440 - val_loss: 0.2609\n",
      "Epoch 35/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 0.5502 - val_loss: 0.1567\n",
      "Epoch 36/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 0.5152 - val_loss: 0.1782\n",
      "Epoch 37/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 0.4982 - val_loss: 0.0848\n",
      "Epoch 38/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 0.4633 - val_loss: 0.2100\n",
      "Epoch 39/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 0.4897 - val_loss: 0.1168\n",
      "Epoch 40/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 0.5084 - val_loss: 0.0519\n",
      "Epoch 41/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 0.4709 - val_loss: 0.0332\n",
      "Epoch 42/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 0.4764 - val_loss: 0.0675\n",
      "Epoch 43/50\n",
      "3246/3246 [==============================] - 5s 1ms/step - loss: 0.4721 - val_loss: 0.0540\n",
      "Epoch 44/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 0.4732 - val_loss: 0.0458\n",
      "Epoch 45/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 0.4791 - val_loss: 0.1054\n",
      "Epoch 46/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 0.4554 - val_loss: 0.0630\n",
      "Epoch 47/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 0.4707 - val_loss: 0.0363\n",
      "Epoch 48/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 0.4637 - val_loss: 0.0276\n",
      "Epoch 49/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 0.4553 - val_loss: 0.1088\n",
      "Epoch 50/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 0.4256 - val_loss: 0.0306\n",
      "compile time: 0.03125309944152832\n",
      "Train on 3246 samples, validate on 171 samples\n",
      "Epoch 1/50\n",
      "3246/3246 [==============================] - 8s 2ms/step - loss: 15.2418 - val_loss: 1.6168\n",
      "Epoch 2/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 3.1083 - val_loss: 1.8917\n",
      "Epoch 3/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 3.1777 - val_loss: 1.6777\n",
      "Epoch 4/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 2.7799 - val_loss: 1.0564\n",
      "Epoch 5/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 2.9379 - val_loss: 1.3539\n",
      "Epoch 6/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 2.4241 - val_loss: 3.6285\n",
      "Epoch 7/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 2.2674 - val_loss: 0.7725\n",
      "Epoch 8/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 2.3577 - val_loss: 1.0319\n",
      "Epoch 9/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 1.8719 - val_loss: 0.6971\n",
      "Epoch 10/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 1.9202 - val_loss: 1.0702\n",
      "Epoch 11/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 1.7050 - val_loss: 0.7420\n",
      "Epoch 12/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 1.8589 - val_loss: 1.0373\n",
      "Epoch 13/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 1.6912 - val_loss: 0.6261\n",
      "Epoch 14/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 1.3747 - val_loss: 0.8124\n",
      "Epoch 15/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 1.6734 - val_loss: 1.1200\n",
      "Epoch 16/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 1.4036 - val_loss: 0.7009\n",
      "Epoch 17/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 1.4875 - val_loss: 0.7459\n",
      "Epoch 18/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 1.3894 - val_loss: 1.0483\n",
      "Epoch 19/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 1.4309 - val_loss: 0.7987\n",
      "Epoch 20/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 1.2590 - val_loss: 0.8807\n",
      "Epoch 21/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 1.4480 - val_loss: 1.3245\n",
      "Epoch 22/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 1.3384 - val_loss: 1.0386\n",
      "Epoch 23/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 1.3154 - val_loss: 0.6463\n",
      "Epoch 24/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 1.2746 - val_loss: 0.4231\n",
      "Epoch 25/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 1.3537 - val_loss: 0.4352\n",
      "Epoch 26/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 1.2582 - val_loss: 0.4435\n",
      "Epoch 27/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 1.2457 - val_loss: 0.3771\n",
      "Epoch 28/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 1.1266 - val_loss: 0.5347\n",
      "Epoch 29/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3246/3246 [==============================] - 4s 1ms/step - loss: 1.2012 - val_loss: 1.4947\n",
      "Epoch 30/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 1.1546 - val_loss: 0.4533\n",
      "Epoch 31/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 1.1246 - val_loss: 0.7683\n",
      "Epoch 32/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 1.1265 - val_loss: 0.4165\n",
      "Epoch 33/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 0.9893 - val_loss: 0.3598\n",
      "Epoch 34/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 1.1654 - val_loss: 0.4754\n",
      "Epoch 35/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 1.1455 - val_loss: 0.4372\n",
      "Epoch 36/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 0.8271 - val_loss: 0.9454\n",
      "Epoch 37/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 1.1565 - val_loss: 0.6916\n",
      "Epoch 38/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 1.1303 - val_loss: 0.4607\n",
      "Epoch 39/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 1.0062 - val_loss: 0.4699\n",
      "Epoch 40/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 0.9393 - val_loss: 0.4681\n",
      "Epoch 41/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 1.0133 - val_loss: 0.3229\n",
      "Epoch 42/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 0.9766 - val_loss: 0.3499\n",
      "Epoch 43/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 0.9181 - val_loss: 0.2873\n",
      "Epoch 44/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 0.8822 - val_loss: 0.2361\n",
      "Epoch 45/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 1.0054 - val_loss: 0.3725\n",
      "Epoch 46/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 0.8941 - val_loss: 0.5557\n",
      "Epoch 47/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 0.8970 - val_loss: 0.4319\n",
      "Epoch 48/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 0.9234 - val_loss: 0.3728\n",
      "Epoch 49/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 0.9636 - val_loss: 0.1836\n",
      "Epoch 50/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 0.8977 - val_loss: 0.8682\n",
      "compile time: 0.03325057029724121\n",
      "Train on 3246 samples, validate on 171 samples\n",
      "Epoch 1/50\n",
      "3246/3246 [==============================] - 8s 3ms/step - loss: 16.1167 - val_loss: 1.7275\n",
      "Epoch 2/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 3.1730 - val_loss: 2.4567\n",
      "Epoch 3/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 3.1466 - val_loss: 1.8226\n",
      "Epoch 4/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 2.9742 - val_loss: 1.7135\n",
      "Epoch 5/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 2.9052 - val_loss: 1.4393\n",
      "Epoch 6/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 2.6122 - val_loss: 1.0251\n",
      "Epoch 7/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 2.5300 - val_loss: 0.7835\n",
      "Epoch 8/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 2.2795 - val_loss: 1.1820\n",
      "Epoch 9/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 1.8841 - val_loss: 1.9202\n",
      "Epoch 10/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 2.1151 - val_loss: 0.5050\n",
      "Epoch 11/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 1.5688 - val_loss: 0.4779\n",
      "Epoch 12/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 1.4697 - val_loss: 1.6405\n",
      "Epoch 13/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 1.8382 - val_loss: 0.3601\n",
      "Epoch 14/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 1.6580 - val_loss: 0.4891\n",
      "Epoch 15/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 1.2784 - val_loss: 0.3658\n",
      "Epoch 16/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 1.3960 - val_loss: 1.8144\n",
      "Epoch 17/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 1.4748 - val_loss: 0.3291\n",
      "Epoch 18/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 1.3413 - val_loss: 1.4485\n",
      "Epoch 19/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 1.3415 - val_loss: 0.3222\n",
      "Epoch 20/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 1.5007 - val_loss: 0.4602\n",
      "Epoch 21/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 1.2005 - val_loss: 0.2187\n",
      "Epoch 22/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 1.3047 - val_loss: 0.2725\n",
      "Epoch 23/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 1.2142 - val_loss: 0.2917\n",
      "Epoch 24/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 1.1077 - val_loss: 0.3991\n",
      "Epoch 25/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 1.4098 - val_loss: 0.3331\n",
      "Epoch 26/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 1.1280 - val_loss: 0.4572\n",
      "Epoch 27/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 1.2156 - val_loss: 0.3600\n",
      "Epoch 28/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 1.0816 - val_loss: 0.1959\n",
      "Epoch 29/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 1.0516 - val_loss: 0.6702\n",
      "Epoch 30/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 1.3687 - val_loss: 0.2760\n",
      "Epoch 31/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 1.0706 - val_loss: 0.2295\n",
      "Epoch 32/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 1.0417 - val_loss: 1.4642\n",
      "Epoch 33/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 1.3912 - val_loss: 0.2142\n",
      "Epoch 34/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 1.0622 - val_loss: 0.1554\n",
      "Epoch 35/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 1.0694 - val_loss: 0.3808\n",
      "Epoch 36/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 1.0184 - val_loss: 0.6171\n",
      "Epoch 37/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 1.1078 - val_loss: 1.1285\n",
      "Epoch 38/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 1.1226 - val_loss: 0.2453\n",
      "Epoch 39/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 0.9461 - val_loss: 0.7302\n",
      "Epoch 40/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 0.9789 - val_loss: 0.8679\n",
      "Epoch 41/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 0.9077 - val_loss: 0.4545\n",
      "Epoch 42/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 1.0667 - val_loss: 0.2199\n",
      "Epoch 43/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 0.9540 - val_loss: 0.2475\n",
      "Epoch 44/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 1.0271 - val_loss: 0.3053\n",
      "Epoch 45/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 0.8087 - val_loss: 0.7496\n",
      "Epoch 46/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 0.9946 - val_loss: 0.2398\n",
      "Epoch 47/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 1.0418 - val_loss: 0.4414\n",
      "Epoch 48/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 0.9998 - val_loss: 0.4656\n",
      "Epoch 49/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 1.0618 - val_loss: 0.1239\n",
      "Epoch 50/50\n",
      "3246/3246 [==============================] - 4s 1ms/step - loss: 0.7788 - val_loss: 0.1428\n",
      "finish the 5205 place\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.models import load_model\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def load_data1(filename, seq_len, place):\n",
    "    set_consumption = pd.read_csv(filename)\n",
    "    data = set_consumption[str(place)].values\n",
    "    sequence_length = seq_len + 1\n",
    "    result = []\n",
    "    for index in range(len(data) - sequence_length):\n",
    "        result.append(data[index : index + sequence_length])\n",
    "    result = np.array(result)\n",
    "    #划分train、test\n",
    "    row1 = int(0.8 * len(result))\n",
    "    train = result[:row1, :]\n",
    "    x_train = train[:, :-1]\n",
    "    y_train = train[:, -1]\n",
    "\n",
    "    test = result[row1:, :]\n",
    "    x_test = test[:, :-1]\n",
    "    y_test = test[:, -1]\n",
    "\n",
    "    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
    "    x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))\n",
    "\n",
    "    return [x_train, y_train, x_test, y_test]\n",
    "\n",
    "def load_data2(filename_set, filename_tra, seq_len, place):\n",
    "    set_consumption = pd.read_csv(filename_set)\n",
    "    set_data = set_consumption[str(place)].values\n",
    "\n",
    "    traffic = pd.read_csv(filename_tra)\n",
    "    tra_data = traffic[str(place)].values\n",
    "\n",
    "    sequence_length = seq_len + 1\n",
    "    result1 = []\n",
    "    for index in range(len(set_data) - sequence_length):\n",
    "        result2 = []\n",
    "        for j in range(index,index+sequence_length):\n",
    "            result2.append(set_data[j])\n",
    "            result2.append(tra_data[j])\n",
    "        result2 = np.array(result2)\n",
    "        result1.append(result2)\n",
    "    result1 = np.array(result1)\n",
    "    #划分train、test\n",
    "    row1 = int(0.8 * len(result1))\n",
    "\n",
    "    train = result1[:row1,:]\n",
    "    x_train = train[:, :-2]\n",
    "    y_train = train[:, -2]\n",
    "\n",
    "    test = result1[row1:, :]\n",
    "    x_test = test[:, :-2]\n",
    "    y_test = test[:, -2]\n",
    "\n",
    "    x_train = np.reshape(x_train, (x_train.shape[0], seq_len, 2))\n",
    "    x_test = np.reshape(x_test, (x_test.shape[0], seq_len, 2))\n",
    "\n",
    "    return [x_train, y_train, x_test, y_test]\n",
    "\n",
    "def load_data3(filename_set, filename_tra, filename_day, seq_len, place):\n",
    "    set_consumption = pd.read_csv(filename_set)\n",
    "    set_data = set_consumption[str(place)].values\n",
    "\n",
    "    traffic = pd.read_csv(filename_tra)\n",
    "    tra_data = traffic[str(place)].values\n",
    "    \n",
    "    day = pd.read_csv(filename_day)\n",
    "    day_data = day['0'].values\n",
    "    \n",
    "    sequence_length = seq_len + 1\n",
    "    result1 = []\n",
    "    for index in range(len(set_data) - sequence_length):\n",
    "        result2 = []\n",
    "        for j in range(index,index+sequence_length):\n",
    "            result2.append(set_data[j])\n",
    "            result2.append(tra_data[j])\n",
    "            result2.append(day_data[j])\n",
    "        result2 = np.array(result2)\n",
    "        result1.append(result2)\n",
    "    result1 = np.array(result1)\n",
    "    #划分train、test\n",
    "    row1 = int(0.8 * len(result1))\n",
    "\n",
    "    train = result1[:row1,:]\n",
    "    x_train = train[:, :-3]\n",
    "    y_train = train[:, -3]\n",
    "\n",
    "    test = result1[row1:, :]\n",
    "    x_test = test[:, :-3]\n",
    "    y_test = test[:, -3]\n",
    "\n",
    "    x_train = np.reshape(x_train, (x_train.shape[0], seq_len, 3))\n",
    "    x_test = np.reshape(x_test, (x_test.shape[0], seq_len, 3))\n",
    "\n",
    "    return [x_train, y_train, x_test, y_test]\n",
    "\n",
    "def build_model(type):\n",
    "    model = Sequential()\n",
    "    if type == 1:\n",
    "        model.add(LSTM(47, input_shape=(47,1), return_sequences = True))\n",
    "        model.add(Dropout(0.2))\n",
    "    elif type == 2:\n",
    "        model.add(LSTM(47, input_shape=(47,2), return_sequences = True))\n",
    "        model.add(Dropout(0.2))\n",
    "    elif type == 3:\n",
    "        model.add(LSTM(47, input_shape=(47,3), return_sequences = True))\n",
    "        model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(LSTM(100,return_sequences = False))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(output_dim=1))\n",
    "    model.add(Activation('linear'))\n",
    "\n",
    "    start = time.time()\n",
    "    model.compile(loss = 'mse', optimizer = 'rmsprop')\n",
    "    print('compile time:',time.time() - start)\n",
    "    return model\n",
    "\n",
    "def predict(model,data):\n",
    "    predicted = model.predict(data)\n",
    "    return predicted\n",
    "\n",
    "def plot_results_multiple(predicted_data1, predicted_data2, predicted_data3, true_data, place_id):\n",
    "    x = [i for i in range(len(true_data))]\n",
    "    y1 = predicted_data1\n",
    "    y = true_data\n",
    "    y2 = predicted_data2\n",
    "    y3 = predicted_data3\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(x, y, label='True Data')\n",
    "    ax2 = ax.twinx()\n",
    "    ax2.plot(x,y2,'r')\n",
    "    ax1 = ax.twinx()\n",
    "    ax1.plot(x,y1,'y')\n",
    "    ax3 = ax.twinx()\n",
    "    ax3.plot(x,y3,'g')\n",
    "    plt.savefig('C:/program2/tensorflow/data/lstm_3_data/plot_results_'+str(place_id)+'.jpg')\n",
    "    #plt.show()\n",
    "\n",
    "def error(pre1,pre2,pre3,true_data):\n",
    "    mae_score_pre1 = mean_absolute_error(true_data,pre1)\n",
    "    mse_score_pre1 = mean_squared_error(true_data,pre1)\n",
    "    mae_score_pre2 = mean_absolute_error(true_data,pre2)\n",
    "    mse_score_pre2 = mean_squared_error(true_data,pre2)\n",
    "    mae_score_pre3 = mean_absolute_error(true_data,pre3)\n",
    "    mse_score_pre3 = mean_squared_error(true_data,pre3)\n",
    "    return [mae_score_pre1,mse_score_pre1,mae_score_pre2,mse_score_pre2,mae_score_pre3,mse_score_pre3]\n",
    "\n",
    "def train(place):\n",
    "    x_train1, y_train1, x_test1, y_test1 = load_data1(filename_set,47,place)\n",
    "    x_train2, y_train2, x_test2, y_test2 = load_data2(filename_set,filename_tra,47,place)\n",
    "    x_train3, y_train3, x_test3, y_test3 = load_data3(filename_set,filename_tra, filename_day, 47, place)\n",
    "    model1 = build_model(1)\n",
    "    model1.fit(x_train1,y_train1,batch_size=256,nb_epoch=50,validation_split=0.05)\n",
    "    model2 = build_model(2)\n",
    "    model2.fit(x_train2,y_train2,batch_size=256,nb_epoch=50,validation_split=0.05)\n",
    "    model3 = build_model(3)\n",
    "    model3.fit(x_train3,y_train3,batch_size=256,nb_epoch=50,validation_split=0.05)\n",
    "    model1.save('C:/program2/tensorflow/data/lstm_1_data/model1_'+str(place)+'.h5')\n",
    "    model2.save('C:/program2/tensorflow/data/lstm_2_data/model2_'+str(place)+'.h5')\n",
    "    model3.save('C:/program2/tensorflow/data/lstm_3_data/model3_'+str(place)+'.h5')\n",
    "    predict_values1 = predict(model1,x_test1)\n",
    "    predict_values2 = predict(model2,x_test2)\n",
    "    predict_values3 = predict(model3,x_test3)\n",
    "    return predict_values1,predict_values2,predict_values3,y_test1\n",
    "\n",
    "if __name__=='__main__':\n",
    "    global filename_tra\n",
    "    global filename_set\n",
    "    filename_pla_id = 'C:/program2/tensorflow/data/traffic_corr_id.csv'\n",
    "    filename_tra = 'C:/program2/tensorflow/data/traffic_de_noising_nov.csv'\n",
    "    filename_set = 'C:/program2/tensorflow/data/set_consumption_nov.csv'\n",
    "    #filename_err = 'C:/program2/tensorflow/data/lstm_2_data/error_conclude.csv'\n",
    "    filename_day = 'C:/program2/tensorflow/data/day_data_nov.csv'\n",
    "\n",
    "    #model2 = load_model('C:\\program2\\tensorflow\\data\\lstm_1_datamodel2_5585.h5')\n",
    "    #model1 = load_model('model1_5585.h5')\n",
    "    \n",
    "    #数据初始化\n",
    "    error_conclu = pd.DataFrame()\n",
    "    place_id = pd.read_csv(filename_pla_id)\n",
    "    place_id = place_id['0'].values\n",
    "    #traffic_columns = pd.read_csv('C:/program2/tensorflow/data/traffic_columns.csv')\n",
    "    #traffic_columns = traffic_columns['0'].values\n",
    "    #set_columns = pd.read_csv('C:/program2/tensorflow/data/set_columns.csv')\n",
    "    #set_columns = set_columns['0'].values\n",
    "\n",
    "    #统计每个地点的预测情况\n",
    "    for i in range(49,len(place_id)):\n",
    "        #if 'id_'+str(place_id[i]) not in traffic_columns or place_id[i] not in set_columns:\n",
    "        #    print('the place '+str(place_id[i])+' is missed')\n",
    "        #    continue\n",
    "        \n",
    "        predict_values1,predict_values2,predict_values3,y_test1 = train(place_id[i])\n",
    "        mae1,mse1,mae2,mse2,mae3,mse3 = error(predict_values1,predict_values2,predict_values3,y_test1)        #能不能合并成一句话 []\n",
    "        error_conclu[str(place_id[i])] = [mae1,mse1,mae2,mse2,mae3,mse3]\n",
    "        plot_results_multiple(predict_values1, predict_values2, predict_values3, y_test1, place_id[i])\n",
    "        print('finish the '+str(place_id[i])+' place')\n",
    "        error_conclu.to_csv('C:/program2/tensorflow/data/lstm_3_data/err_conclude/de_noising/'+str(place_id[i])+'.csv')\n",
    "        if i == 49:\n",
    "            break\n",
    "    error_conclu.to_csv('C:/program2/tensorflow/data/lstm_3_data/err_conclude/de_noising/high_corr.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  277,  2502,  2737,  2738,  2853,  2855,  2971,  2972,  2973,\n",
       "        3089,  3090,  3191,  3192,  3322,  4031,  4148,  4265,  4266,\n",
       "        4384,  4385,  4500,  4501,  4616,  4617,  4618,  4619,  4731,\n",
       "        4733,  4734,  4735,  4736,  4848,  4849,  4850,  4851,  4852,\n",
       "        4966,  4967,  4968,  4970,  5083,  5084,  5085,  5087,  5200,\n",
       "        5201,  5202,  5203,  5204,  5205,  5206,  5207,  5317,  5318,\n",
       "        5319,  5320,  5323,  5324,  5433,  5434,  5435,  5436,  5437,\n",
       "        5438,  5439,  5441,  5550,  5551,  5552,  5553,  5554,  5667,\n",
       "        5668,  5669,  5676,  5783,  5784,  6019,  6136,  6137,   276,\n",
       "         393,   978,   979,   982,  1096,  1097,  1331,  1566,  1683,\n",
       "        1684,  1800,  1915,  1916,  1917,  2022,  2032,  2033,  2034,\n",
       "        2139,  2148,  2149,  2150,  2151,  2256,  2258,  2264,  2265,\n",
       "        2266,  2267,  2268,  2373,  2381,  2382,  2383,  2384,  2385,\n",
       "        2491,  2501,  2607,  2608,  2619,  2623,  2725,  2736,  2845,\n",
       "        2960,  2961,  2962,  2963,  2969,  2970,  3073,  3076,  3077,\n",
       "        3078,  3079,  3080,  3081,  3086,  3087,  3193,  3194,  3196,\n",
       "        3197,  3198,  3202,  3203,  3204,  3205,  3206,  3208,  3209,\n",
       "        3210,  3308,  3309,  3310,  3311,  3312,  3319,  3320,  3321,\n",
       "        3323,  3324,  3325,  3326,  3327,  3328,  3329,  3330,  3427,\n",
       "        3428,  3431,  3437,  3438,  3439,  3440,  3441,  3442,  3443,\n",
       "        3444,  3544,  3547,  3548,  3554,  3555,  3558,  3559,  3561,\n",
       "        3562,  3664,  3665,  3666,  3667,  3672,  3679,  3778,  3779,\n",
       "        3780,  3781,  3782,  3792,  3794,  3795,  3796,  3895,  3896,\n",
       "        3899,  3910,  3911,  3912,  3913,  4017,  4029,  4030,  4134,\n",
       "        4135,  4145,  4146,  4147,  4252,  4261,  4262,  4263,  4264,\n",
       "        4268,  4269,  4369,  4370,  4379,  4380,  4381,  4382,  4383,\n",
       "        4386,  4488,  4497,  4498,  4499,  4502,  4605,  4614,  4615,\n",
       "        4642,  4702,  4721,  4722,  4723,  4724,  4732,  4753,  4759,\n",
       "        4760,  4838,  4839,  4840,  4841,  4875,  4876,  4877,  4957,\n",
       "        4965,  4969,  4991,  4993,  4994,  4995,  4996,  5073,  5074,\n",
       "        5077,  5078,  5086,  5089,  5090,  5108,  5109,  5110,  5190,\n",
       "        5191,  5194,  5208,  5226,  5227,  5228,  5305,  5307,  5308,\n",
       "        5309,  5310,  5312,  5313,  5314,  5315,  5321,  5425,  5426,\n",
       "        5428,  5429,  5440,  5442,  5543,  5545,  5546,  5549,  5555,\n",
       "        5556,  5557,  5558,  5559,  5661,  5662,  5663,  5666,  5672,\n",
       "        5674,  5675,  5779,  5780,  5785,  5788,  5793,  5800,  5897,\n",
       "        5900,  5901,  5903,  5904,  5910,  6016,  6017,  6018,  6020,\n",
       "        6021,  6109,  6110,  6130,  6134,  6135,  6225,  6226,  6247,\n",
       "        6249,  6250,  6251,  6252,  6253,  6365,  6366,  6367,  6370,\n",
       "        6371,  6372,  6373,  6374,  6375,  6376,  6378,  6379,  6482,\n",
       "        6483,  6484,  6489,  6490,  6492,  6493,  6494,  6495,  6598,\n",
       "        6601,  6608,  6611,  6717,  6718,  6727,  6841,  7080,  7184,\n",
       "        7302,  7418,  7652,  7770,  7928,  8004,  8005,  8006,  8042,\n",
       "        8121,  8122,  8123,  8124,  8157,  8158,  8159,  8160,  8238,\n",
       "        8239,  8241,  8242,  8267,  8277,  8358,  8361,  8383,  8384,\n",
       "        8386,  8395,  8474,  8502,  8566,  8592,  9454,  9653, 10121,\n",
       "         272,   273,   389,   390,   506,   507,   624,   625,   626,\n",
       "         742,   743,   859,   860,   861,   862,   976,   977,   980,\n",
       "        1098,  1099,  1214,  1332,  1448,  1449,  1450,  1451,  1567,\n",
       "        1676,  1793,  1798,  1799,  1910,  1913,  1921,  1922,  2028,\n",
       "        2029,  2030,  2031,  2040,  2145,  2147,  2155,  2156,  2253,\n",
       "        2271,  2272,  2380,  2388,  2390,  2391,  2468,  2469,  2500,\n",
       "        2505,  2506,  2507,  2508,  2622,  2703,  2739,  2822,  2843,\n",
       "        2854,  2856,  2857,  2937,  2938,  2939,  2974,  3053,  3054,\n",
       "        3055,  3056,  3072,  3091,  3170,  3195,  3207,  3213,  3214,\n",
       "        3291,  3292,  3331,  3332,  3410,  3411,  3429,  3445,  3450,\n",
       "        3545,  3673,  3678,  3783,  3784,  3785,  3786,  3787,  3790,\n",
       "        3900,  3901,  3902,  3903,  3908,  3909,  4019,  4020,  4026,\n",
       "        4027,  4028,  4037,  4136,  4137,  4144,  4191,  4253,  4254,\n",
       "        4270,  4274,  4275,  4371,  4387,  4391,  4392,  4394,  4400,\n",
       "        4401,  4402,  4423,  4424,  4468,  4506,  4507,  4508,  4516,\n",
       "        4518,  4519,  4520,  4538,  4539,  4540,  4541,  4585,  4586,\n",
       "        4587,  4588,  4602,  4622,  4623,  4624,  4632,  4633,  4634,\n",
       "        4635,  4636,  4640,  4656,  4703,  4704,  4705,  4719,  4739,\n",
       "        4740,  4741,  4751,  4752,  4754,  4758,  4770,  4773,  4819,\n",
       "        4821,  4822,  4823,  4836,  4856,  4857,  4858,  4870,  4871,\n",
       "        4872,  4933,  4934,  4941,  4942,  4956,  4973,  4974,  4975,\n",
       "        4987,  4988,  4989,  4990,  4992,  5059,  5060,  5068,  5071,\n",
       "        5079,  5091,  5104,  5105,  5106,  5107,  5111,  5185,  5189,\n",
       "        5195,  5196,  5213,  5220,  5221,  5222,  5223,  5224,  5225,\n",
       "        5229,  5302,  5303,  5311,  5316,  5326,  5329,  5330,  5338,\n",
       "        5339,  5340,  5341,  5342,  5343,  5344,  5345,  5346,  5424,\n",
       "        5430,  5431,  5432,  5446,  5447,  5448,  5450,  5455,  5456,\n",
       "        5457,  5458,  5459,  5461,  5462,  5463,  5522,  5523,  5563,\n",
       "        5565,  5566,  5567,  5575,  5579,  5580,  5581,  5638,  5639,\n",
       "        5640,  5670,  5679,  5683,  5684,  5685,  5696,  5698,  5756,\n",
       "        5757,  5758,  5786,  5787,  5789,  5791,  5792,  5801,  5874,\n",
       "        5898,  5902,  5909,  5911,  5912,  5913,  5991,  5993,  6013,\n",
       "        6024,  6028,  6029,  6108,  6133,  6138,  6186,  6228,  6256,\n",
       "        6257,  6258,  6260,  6261,  6264,  6302,  6368,  6369,  6485,\n",
       "        6486,  6487,  6488,  6496,  6497,  6596,  6597,  6599,  6600,\n",
       "        6602,  6603,  6604,  6605,  6612,  6613,  6614,  6615,  6616,\n",
       "        6617,  6654,  6697,  6713,  6714,  6720,  6721,  6722,  6724,\n",
       "        6726,  6729,  6730,  6731,  6732,  6815,  6838,  6842,  6845,\n",
       "        6847,  6848,  6849,  6932,  6948,  6949,  6955,  6959,  6961,\n",
       "        6962,  6963,  6964,  6965,  7066,  7081,  7114,  7182,  7183,\n",
       "        7230,  7284,  7298,  7299,  7300,  7301,  7345,  7401,  7402,\n",
       "        7415,  7416,  7425,  7427,  7533,  7534,  7535,  7543,  7544,\n",
       "        7545,  7636,  7649,  7650,  7651,  7660,  7661,  7768,  7769,\n",
       "        7811,  7884,  7885,  7886,  8002,  8003,  8041,  8101,  8118,\n",
       "        8119,  8145,  8147,  8148,  8149,  8150,  8154,  8155,  8235,\n",
       "        8236,  8237,  8262,  8263,  8264,  8265,  8266,  8268,  8276,\n",
       "        8353,  8354,  8355,  8356,  8357,  8359,  8382,  8385,  8454,\n",
       "        8455,  8469,  8471,  8473,  8477,  8500,  8503,  8588,  8590,\n",
       "        8591,  8705,  8706,  8828,  8859,  8860,  8940,  9041,  9042,\n",
       "        9044,  9057,  9058,  9059,  9174,  9175,  9176,  9177,  9274,\n",
       "        9275,  9288,  9289,  9290,  9292,  9293,  9337,  9338,  9390,\n",
       "        9391,  9403,  9404,  9405,  9406,  9408,  9410,  9411,  9418,\n",
       "        9452,  9508,  9519,  9520,  9523,  9526,  9527,  9535,  9652,\n",
       "        9765,  9769,  9877,  9881,  9882,  9997,  9998,  9999, 10004,\n",
       "       10114, 10115, 10116, 10222, 10232, 10465, 10466, 10582, 10583],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = pd.read_csv('C:/program2/tensorflow/data/traffic_corr_id.csv')\n",
    "a = a['0'].values\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind = list(a).index(5205)\n",
    "ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
